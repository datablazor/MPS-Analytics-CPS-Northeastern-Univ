{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Tokenizer_Sentiment_Classification_IMDB_MovieReviews.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9OcLx84Bfhj"
      },
      "source": [
        "This example shows how to use BERT Tokenizer for text classification <br>\n",
        "(We can extend this example to show BERT Tokenizer + Bert Embeddings usage for text classification) \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLYi3TD9H73z",
        "outputId": "c2f53bd4-fcda-4ad6-c377-a81771b4a95b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.14.7)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.9.7)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMoiv-FGIcHU"
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68TApq23J7W8",
        "outputId": "f51d213c-196d-44b9-c3d9-05611439b1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls /content/drive/My\\ Drive/DataFiles"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IMDBDataset.csv  Telco-Customer-Churn-IBM.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du4rTt1fIlYd",
        "outputId": "8259153b-b1ac-43b0-f1b4-9135d616f22e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "movie_reviews = pd.read_csv(\"/content/drive/My Drive/DataFiles/IMDBDataset.csv\")\n",
        "\n",
        "movie_reviews.isnull().values.any()\n",
        "\n",
        "movie_reviews.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrfPag1AUS2x",
        "outputId": "532935be-7256-48e0-cd3a-083c9d949867",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        }
      },
      "source": [
        "pd.set_option(\"display.max_rows\", 200)\n",
        "pd.set_option(\"display.max_columns\", 100)\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "\n",
        "movie_reviews.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.&lt;br /&gt;&lt;br /&gt;The first thing that struck me...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire p...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue i...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet &amp; his parents are fighting all the time.&lt;br /&gt;&lt;br /&gt;This movie is slower than a soap opera... and suddenl...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what mone...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it's not preachy or boring. It just never gets old, despite my having seen it some 15 o...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I sure would like to see a resurrection of a up dated Seahunt series with the tech they have today it would bring back the kid excitement in me.I grew up on black and white TV and Seahunt with Gun...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>This show was an amazing, fresh &amp; innovative idea in the 70's when it first aired. The first 7 or 8 years were brilliant, but things dropped off after that. By 1990, the show was not really funny ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Encouraged by the positive comments about this film on here I was looking forward to watching this film. Bad mistake. I've seen 950+ films and this is truly one of the worst of them - it's awful i...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>If you like original gut wrenching laughter you will like this movie. If you are young or old then you will love this movie, hell even my mom liked it.&lt;br /&gt;&lt;br /&gt;Great Camp!!!</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                    review  \\\n",
              "0  One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me...   \n",
              "1  A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire p...   \n",
              "2  I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue i...   \n",
              "3  Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenl...   \n",
              "4  Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what mone...   \n",
              "5  Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it's not preachy or boring. It just never gets old, despite my having seen it some 15 o...   \n",
              "6  I sure would like to see a resurrection of a up dated Seahunt series with the tech they have today it would bring back the kid excitement in me.I grew up on black and white TV and Seahunt with Gun...   \n",
              "7  This show was an amazing, fresh & innovative idea in the 70's when it first aired. The first 7 or 8 years were brilliant, but things dropped off after that. By 1990, the show was not really funny ...   \n",
              "8  Encouraged by the positive comments about this film on here I was looking forward to watching this film. Bad mistake. I've seen 950+ films and this is truly one of the worst of them - it's awful i...   \n",
              "9                         If you like original gut wrenching laughter you will like this movie. If you are young or old then you will love this movie, hell even my mom liked it.<br /><br />Great Camp!!!   \n",
              "\n",
              "  sentiment  \n",
              "0  positive  \n",
              "1  positive  \n",
              "2  positive  \n",
              "3  negative  \n",
              "4  positive  \n",
              "5  positive  \n",
              "6  positive  \n",
              "7  negative  \n",
              "8  negative  \n",
              "9  positive  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K512NSi-ZxeG"
      },
      "source": [
        "**Pre-Processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB2f8BlqJ9jC"
      },
      "source": [
        "## Pre-process the data\n",
        "def preprocess_text(sen):\n",
        "    # Removing html tags\n",
        "    sentence = remove_tags(sen)\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "    # Single character removal (\\s is a white space, \\s+ one is 1/more white spaces)\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3lRBENZKE5n"
      },
      "source": [
        "## Remove all html tags\n",
        "import re\n",
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def remove_tags(text):\n",
        "    return TAG_RE.sub('', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1DsMb3SKIjb"
      },
      "source": [
        "## Create i/p variable \n",
        "reviews = []\n",
        "sentences = list(movie_reviews['review'])\n",
        "for sen in sentences:\n",
        "    reviews.append(preprocess_text(sen))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dW3IcNdKZXS",
        "outputId": "2a877461-95ee-417f-d23e-2a44786ea6b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(movie_reviews.columns.values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['review' 'sentiment']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnDuyt7wKb6s",
        "outputId": "eedccd27-2fa9-443e-c9c4-5c82129b0fc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "movie_reviews.sentiment.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['positive', 'negative'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okJAN7vQKeGc"
      },
      "source": [
        "## Create o/p variable\n",
        "import numpy as np\n",
        "\n",
        "y = movie_reviews['sentiment']\n",
        "\n",
        "y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, y)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgrdwPv7KgGT",
        "outputId": "61cd64d4-d807-46e4-b1b5-f5a9e25d96ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(reviews[10])\n",
        "print(y[10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Phil the Alien is one of those quirky films where the humour is based around the oddness of everything rather than actual punchlines At first it was very odd and pretty funny but as the movie progressed didn find the jokes or oddness funny anymore Its low budget film thats never problem in itself there were some pretty interesting characters but eventually just lost interest imagine this film would appeal to stoner who is currently partaking For something similar but better try Brother from another planet \n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_N-FDfxZrvY"
      },
      "source": [
        "**Creating a BERT Tokenizer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbQdrFvXekTy"
      },
      "source": [
        "*Special Tokens used in BERT*\n",
        "\n",
        "[CLS] : The first token of every sequence. A classification token which is normally used in conjunction with a softmax layer for classification tasks. For anything else, it can be safely ignored.\n",
        "\n",
        "[SEP] : A sequence delimiter token which was used at pre-training for sequence-pair tasks (i.e. Next sentence prediction). Must be used when sequence pair tasks are required. When a single sequence is used it is just appended at the end.\n",
        "\n",
        "[MASK] : Token used for masked words. Only used for pre-training.\n",
        "\n",
        "**Transfer learning** is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9sE6rF4KmzE"
      },
      "source": [
        "## Tokenization refers to dividing a sentence into individual words\n",
        "\n",
        "# First create BertTokenizer object from bert.bert_tokenization module\n",
        "# Create BERT embedding layer from hub.KerasLayer; tfhub is a repository for pre-trained models (if trainable = False, we will not be further training the BERT embedding)\n",
        "# Create BERT vocabulary file and the the lower case variables to BertTokenizer object.\n",
        "\n",
        "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=False)\n",
        "\n",
        "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "\n",
        "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbO1GtmYPn0n",
        "outputId": "64981138-35b9-451b-ce79-15c377b0a98a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## any words that are not in the vocab(Out-Of-Vocabulary(OOV) words) will be considered as a separate token and will be pre-fixed with a ##\n",
        "tokenizer.tokenize(\"don't be so judgmental\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['don', \"'\", 't', 'be', 'so', 'judgment', '##al']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDlnoVH_PplQ",
        "outputId": "4f5f7844-e903-4d84-b28b-63c2268a441a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## token embeddings are the vocabulary IDs for each of the tokens.\n",
        "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"dont be so judgmental\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2123, 2102, 2022, 2061, 8689, 2389]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua0DsqYrPrvD"
      },
      "source": [
        "def tokenize_reviews(text_reviews):\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azxPXS17l9ZR",
        "outputId": "990766a7-31b7-4eba-e80c-15a5e9189d18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "reviews[10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Phil the Alien is one of those quirky films where the humour is based around the oddness of everything rather than actual punchlines At first it was very odd and pretty funny but as the movie progressed didn find the jokes or oddness funny anymore Its low budget film thats never problem in itself there were some pretty interesting characters but eventually just lost interest imagine this film would appeal to stoner who is currently partaking For something similar but better try Brother from another planet '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsczhkHwPuYU"
      },
      "source": [
        "## Convert tokens to ids in i/p data\n",
        "tokenized_reviews = [tokenize_reviews(review) for review in reviews]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkogCvKriQir",
        "outputId": "64583d68-62d9-4eff-f84b-29137b9fae91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(tokenized_reviews)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1zhr1HMmm0E",
        "outputId": "80edfbda-e771-4d8d-d6c2-6a1a143b90fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(reviews[10])\n",
        "print(tokenized_reviews[10])\n",
        "\n",
        "## differenc between the lengths are due to OOVs\n",
        "print(len(reviews[10].split()))\n",
        "print(len(tokenizer.tokenize(reviews[10])))\n",
        "print(len(tokenized_reviews[10]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Phil the Alien is one of those quirky films where the humour is based around the oddness of everything rather than actual punchlines At first it was very odd and pretty funny but as the movie progressed didn find the jokes or oddness funny anymore Its low budget film thats never problem in itself there were some pretty interesting characters but eventually just lost interest imagine this film would appeal to stoner who is currently partaking For something similar but better try Brother from another planet \n",
            "[6316, 1996, 7344, 2003, 2028, 1997, 2216, 21864, 15952, 3152, 2073, 1996, 17211, 2003, 2241, 2105, 1996, 5976, 2791, 1997, 2673, 2738, 2084, 5025, 8595, 12735, 2012, 2034, 2009, 2001, 2200, 5976, 1998, 3492, 6057, 2021, 2004, 1996, 3185, 12506, 2134, 2424, 1996, 13198, 2030, 5976, 2791, 6057, 4902, 2049, 2659, 5166, 2143, 2008, 2015, 2196, 3291, 1999, 2993, 2045, 2020, 2070, 3492, 5875, 3494, 2021, 2776, 2074, 2439, 3037, 5674, 2023, 2143, 2052, 5574, 2000, 2962, 2099, 2040, 2003, 2747, 2112, 15495, 2005, 2242, 2714, 2021, 2488, 3046, 2567, 2013, 2178, 4774]\n",
            "86\n",
            "93\n",
            "93\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ujxjij7fhZu7"
      },
      "source": [
        "**Preparing the training data for BERT**\n",
        "\n",
        "The reviews in our dataset have varying lengths. Some reviews are very small while others are very long. To train the model, the input sentences should be of equal length. To create sentences of equal length, one way is to pad the shorter sentences by 0s. However, this can result in a sparse matrix contain large number of 0s. \n",
        "Alternatively, we can pad sentences within each batch. Since we will be training the model in batches, we can pad the sentences within the training batch locally depending upon the length of the longest sentence. To do so, we first need to find the length of each sentence.\n",
        "\n",
        "The following script creates a list of lists where each sublist contains tokenized review, the label of the review and the length of the review:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3jiKbtDPv93"
      },
      "source": [
        "reviews_with_len = [[review, y[i], len(review)]\n",
        "                 for i, review in enumerate(tokenized_reviews)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWb926vfoyBp",
        "outputId": "764ed71a-40f4-419b-c93f-41616d0502f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reviews_with_len[10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[6316,\n",
              "  1996,\n",
              "  7344,\n",
              "  2003,\n",
              "  2028,\n",
              "  1997,\n",
              "  2216,\n",
              "  21864,\n",
              "  15952,\n",
              "  3152,\n",
              "  2073,\n",
              "  1996,\n",
              "  17211,\n",
              "  2003,\n",
              "  2241,\n",
              "  2105,\n",
              "  1996,\n",
              "  5976,\n",
              "  2791,\n",
              "  1997,\n",
              "  2673,\n",
              "  2738,\n",
              "  2084,\n",
              "  5025,\n",
              "  8595,\n",
              "  12735,\n",
              "  2012,\n",
              "  2034,\n",
              "  2009,\n",
              "  2001,\n",
              "  2200,\n",
              "  5976,\n",
              "  1998,\n",
              "  3492,\n",
              "  6057,\n",
              "  2021,\n",
              "  2004,\n",
              "  1996,\n",
              "  3185,\n",
              "  12506,\n",
              "  2134,\n",
              "  2424,\n",
              "  1996,\n",
              "  13198,\n",
              "  2030,\n",
              "  5976,\n",
              "  2791,\n",
              "  6057,\n",
              "  4902,\n",
              "  2049,\n",
              "  2659,\n",
              "  5166,\n",
              "  2143,\n",
              "  2008,\n",
              "  2015,\n",
              "  2196,\n",
              "  3291,\n",
              "  1999,\n",
              "  2993,\n",
              "  2045,\n",
              "  2020,\n",
              "  2070,\n",
              "  3492,\n",
              "  5875,\n",
              "  3494,\n",
              "  2021,\n",
              "  2776,\n",
              "  2074,\n",
              "  2439,\n",
              "  3037,\n",
              "  5674,\n",
              "  2023,\n",
              "  2143,\n",
              "  2052,\n",
              "  5574,\n",
              "  2000,\n",
              "  2962,\n",
              "  2099,\n",
              "  2040,\n",
              "  2003,\n",
              "  2747,\n",
              "  2112,\n",
              "  15495,\n",
              "  2005,\n",
              "  2242,\n",
              "  2714,\n",
              "  2021,\n",
              "  2488,\n",
              "  3046,\n",
              "  2567,\n",
              "  2013,\n",
              "  2178,\n",
              "  4774],\n",
              " 0,\n",
              " 93]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGrUB3-aP2Yj"
      },
      "source": [
        "## Shuffle the dataset to uniformly distribute 1s and 0s in the data\n",
        "import random\n",
        "random.shuffle(reviews_with_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7Jze_W5P5PS"
      },
      "source": [
        "## Sort based on length of review; 3rd item in the sublist i.e. the length of the review\n",
        "reviews_with_len.sort(key=lambda x: x[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw3Ld8uAP7an"
      },
      "source": [
        "## Remove length of the review attribute\n",
        "sorted_reviews_labels = [(review_lab[0], review_lab[1]) for review_lab in reviews_with_len]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNE2Hbqdo-Eo",
        "outputId": "5173d2a9-4b51-4073-ce1f-1592056f4b5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sorted_reviews_labels[3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([2062, 23873, 3993, 2062, 11259, 2172, 2172, 2062, 14888], 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_H4U06KP_ki"
      },
      "source": [
        "## Convert the sorted dataset into a TensorFlow 2.0-compliant input dataset shape.\n",
        "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32, tf.int32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmeT3iP4QBZu"
      },
      "source": [
        "## Padding the dataset for each batch: \n",
        "# Let's use batch size = 32; meaning that after processing 32 reviews, the weights of the neural network will be updated.\n",
        "# To pad the reviews locally with respect to batches, execute the following:\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdKABMwuQJfr",
        "outputId": "0962d46a-bb6a-410c-f8df-0b69ed42820a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print the first batch and see how padding has been applied to it:\n",
        "next(iter(batched_dataset))\n",
        "# first review of the batch(and the rest) is padded with 0s to match with the length of the last review in the batch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(32, 21), dtype=int32, numpy=\n",
              " array([[ 2054,  5896,  2054,  2466,  2054,  6752,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 3078,  5436,  3078,  3257,  3532,  7613,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 3191,  1996,  2338,  5293,  1996,  3185,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 2062, 23873,  3993,  2062, 11259,  2172,  2172,  2062, 14888,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 2023,  3185,  2003,  6659,  2021,  2009,  2038,  2070,  2204,\n",
              "          3896,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 1045,  2876,  9278,  2023,  2028,  2130,  2006,  7922, 12635,\n",
              "          2305,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 8235,  1998,  3048,  4616,  2011,  3419,  2457, 27727,  1998,\n",
              "          2848, 16133,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 1045,  3246,  2023,  2177,  1997,  2143, 11153,  2196,  2128,\n",
              "         15908,  2015,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 7918, 14674,  7662,  2003,  6581,  2003,  2023,  2143,  2002,\n",
              "          3084, 17160,  2450,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 2023,  2003,  2307,  3185,  2205,  2919,  2009,  2003,  2025,\n",
              "          2800,  2006,  2188,  2678,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [11861,  1996, 21442,  6895,  3238,  2515,  2210, 22759,  6198,\n",
              "          1998,  3185,  2087, 12487,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 2017,  2488,  5454,  2703,  2310, 25032,  8913,  8159,  2130,\n",
              "          2065,  2017,  2031,  3427,  2009,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 2053,  7615,  5236,  3185,  3772,  2779,  2030,  4788,  9000,\n",
              "          2053,  3168,  2012,  2035, 13558,  2009,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 1045,  2123,  2113,  2339,  2066,  2023,  3185,  2061,  2092,\n",
              "          2021,  2196,  2131,  5458,  1997,  3666,  2009,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 2074,  2293,  1996,  6970, 13068,  2090,  2048,  2307,  3494,\n",
              "          1997,  2754,  3898,  2310,  3593,  2102,  6287,  5974,     0,\n",
              "             0,     0,     0],\n",
              "        [ 2146, 11771,  1038,  8523,  8458,  6633,  3560,  2196,  2031,\n",
              "          2042,  2061,  5580,  2000,  2156,  4566,  6495,  4897,     0,\n",
              "             0,     0,     0],\n",
              "        [ 7615,  2023,  3185,  2003,  5263,  2003,  6659,  2200, 17727,\n",
              "          3217,  3676,  3468,  2919,  7613,  3257,  2025,  2298,     0,\n",
              "             0,     0,     0],\n",
              "        [ 5587,  2023,  2210, 17070,  2000,  2115,  2862,  1997,  6209,\n",
              "         24945,  2009, 26354, 28394,  2102,  6057,  1998,  2203, 27242,\n",
              "             0,     0,     0],\n",
              "        [ 2023,  2003,  2204,  2143,  2023,  2003,  2200,  6057,  2664,\n",
              "          2044,  2023,  2143,  2045,  2020,  2053,  2204,  8471,  3152,\n",
              "             0,     0,     0],\n",
              "        [ 2235,  3077,  2792,  3425,  2003,  1996,  2190,  2792,  1997,\n",
              "          2235,  3077,  2009,  2026,  5440,  2792,  1997,  2235,  3077,\n",
              "             0,     0,     0],\n",
              "        [ 2235,  3077,  2792,  3425,  2003,  1996,  2190,  2792,  1997,\n",
              "          2235,  3077,  2009,  2026,  5440,  2792,  1997,  2235,  3077,\n",
              "             0,     0,     0],\n",
              "        [ 1037,  7244,  3185,  2009,  2003,  2440,  1997,  6699,  1998,\n",
              "          6919,  3772,  2071,  2031,  2938,  2083,  2009,  2117,  2051,\n",
              "             0,     0,     0],\n",
              "        [ 2023,  2003,  1996, 15764,  3185,  2544,  1997,  8429, 24905,\n",
              "         17988,  7659,  2498,  2021,  2045,  2024,  2053, 13842,  5312,\n",
              "             0,     0,     0],\n",
              "        [ 7078, 10392,  3649,  2360,  2876,  2079,  2023,  2104,  9250,\n",
              "          3185,  1996,  3425,  2009, 17210,  3422,  2009,  2085, 10392,\n",
              "             0,     0,     0],\n",
              "        [ 1037,  2033,  6491, 11124,  6774,  2143,  2008,  5121,  7906,\n",
              "          2115,  3086,  3841, 13196,  2003, 17160,  1998, 26103,  2000,\n",
              "          3422,     0,     0],\n",
              "        [ 6283,  2009,  2007,  2035,  2026,  2108,  5409,  3185,  2412,\n",
              "         10597, 21985,  2393,  2033,  2009,  2001,  2008,  2919,  3404,\n",
              "          2033,     0,     0],\n",
              "        [ 1037,  5790,  1997,  2515,  2025,  4088,  2000,  4671,  2129,\n",
              "         10634,  2139, 24128,  1998, 21660,  2135,  2919,  2023,  3185,\n",
              "          2003,     0,     0],\n",
              "        [ 7244,  2092,  2856, 10828,  1997, 10904,  2402,  2472,  3135,\n",
              "          2293,  2466,  2007, 10958,  8428, 10102,  1999,  1996,  4281,\n",
              "          4276,  3773,     0],\n",
              "        [ 2005,  5760,  7788,  4393,  8808,  2498,  2064, 12826,  2000,\n",
              "          1996, 11056,  3152,  3811, 16755,  2169,  1998,  2296,  2028,\n",
              "          1997,  2068,     0],\n",
              "        [ 2023,  2003,  6659,  3185,  2123,  5949,  2115,  2769,  2006,\n",
              "          2009,  2123,  2130,  3422,  2009,  2005,  2489,  2008,  2035,\n",
              "          2031,  2000,  2360],\n",
              "        [ 2023,  3185,  2097,  2467,  2022,  5934,  1998,  3185,  4438,\n",
              "          2004,  2146,  2004,  2045,  2024,  2145,  2111,  2040,  6170,\n",
              "          3153,  1998,  2552],\n",
              "        [ 7794, 28208, 18224, 15069,  2004, 18801,  2386, 28810,  2143,\n",
              "         15587, 10874,  2029,  2987,  2191,  2172,  1997,  2049,  6980,\n",
              "         10015,  2275,  2039]], dtype=int32)>,\n",
              " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
              " array([0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 0, 0, 1, 1, 0, 1, 0], dtype=int32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_-kM008QN2Q"
      },
      "source": [
        "import math\n",
        "# total number of batches in the dataset\n",
        "TOTAL_BATCHES = math.ceil(len(sorted_reviews_labels) / BATCH_SIZE)\n",
        "# 10% of the batches = test batches\n",
        "TEST_BATCHES = TOTAL_BATCHES // 10\n",
        "\n",
        "batched_dataset.shuffle(TOTAL_BATCHES)\n",
        "\n",
        "test_data = batched_dataset.take(TEST_BATCHES)\n",
        "train_data = batched_dataset.skip(TEST_BATCHES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd3T98bk5Fc4",
        "outputId": "c634316a-5096-4fc9-a0c2-19abbf9a542c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "TOTAL_BATCHES"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1563"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQrFeFRx4XKM"
      },
      "source": [
        "**Creating the Model**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r73qIUE4tg6"
      },
      "source": [
        "An **embedding** is a relatively low-dimensional space into which you can translate high-dimensional vectors. Embeddings make it easier to do machine learning on large inputs like sparse vectors representing words.\n",
        "\n",
        "**Dropout** is easily implemented by randomly selecting nodes to be dropped-out with a given probability (e.g. 10%) each weight update cycle. This is how Dropout regularization is implemented in Keras. Dropout is only used during the training of a model and is not used when evaluating the skill of the model. If the dropout rate is 10%, i.e. one in 10 inputs will be randomly excluded from each update cycle. Use dropout on incoming (visible) as well as hidden units.\n",
        "\n",
        "**Softmax** calculates a probability for every possible class. It is implemented through a neural network layer just before the output layer. The Softmax layer must have the same number of nodes as the output layer.\n",
        "\n",
        "1 **Epoch** = 1 Forward pass + 1 Backward pass for ALL training samples.\n",
        "Batch Size = Number of training samples in 1 Forward/1 Backward pass\n",
        "Number of iterations = Number of passes i.e. 1 Pass = 1 Forward pass + 1 Backward pass <br> Example : If we have 1000 training samples and Batch size is set to 500, it will take 2 iterations to complete 1 Epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fj0UMFvsQQ7g"
      },
      "source": [
        "## this clas inherits from tf.keras.Model class\n",
        "# use 3 Convolutional NN layers\n",
        "class TEXT_MODEL(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocabulary_size,\n",
        "                 embedding_dimensions=128,\n",
        "                 cnn_filters=50,\n",
        "                 dnn_units=512,\n",
        "                 model_output_classes=2,\n",
        "                 dropout_rate=0.1,\n",
        "                 training=False,\n",
        "                 name=\"text_model\"):\n",
        "        super(TEXT_MODEL, self).__init__(name=name)\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocabulary_size,\n",
        "                                          embedding_dimensions)\n",
        "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=2,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=3,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=4,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        if model_output_classes == 2:\n",
        "            self.last_dense = layers.Dense(units=1,\n",
        "                                           activation=\"sigmoid\")\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(units=model_output_classes,\n",
        "                                           activation=\"softmax\")\n",
        "            \n",
        "    # global max pooling is applied to the output of each of the convolutional neural network layer\n",
        "    # The three convolutional neural network layers are concatenated together and their output is fed to the first densely connected neural network. \n",
        "    # The second densely connected neural network is used to predict the output sentiment since it only contains 2 classes\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        l = self.embedding(inputs)\n",
        "        l_1 = self.cnn_layer1(l) \n",
        "        l_1 = self.pool(l_1) \n",
        "        l_2 = self.cnn_layer2(l) \n",
        "        l_2 = self.pool(l_2)\n",
        "        l_3 = self.cnn_layer3(l)\n",
        "        l_3 = self.pool(l_3) \n",
        "        \n",
        "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
        "        concatenated = self.dense_1(concatenated)\n",
        "        concatenated = self.dropout(concatenated, training)\n",
        "        model_output = self.last_dense(concatenated)\n",
        "        \n",
        "        return model_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFcjV-5fQUeD",
        "outputId": "a17e0c2c-d0c2-45b0-aeb5-530801d62304",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "VOCAB_LENGTH = len(tokenizer.vocab)\n",
        "print(VOCAB_LENGTH)\n",
        "EMB_DIM = 200\n",
        "CNN_FILTERS = 100\n",
        "DNN_UNITS = 256\n",
        "OUTPUT_CLASSES = 2\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "NB_EPOCHS = 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E7IBaz5QubK"
      },
      "source": [
        "text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
        "                        embedding_dimensions=EMB_DIM,\n",
        "                        cnn_filters=CNN_FILTERS,\n",
        "                        dnn_units=DNN_UNITS,\n",
        "                        model_output_classes=OUTPUT_CLASSES,\n",
        "                        dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBcNCgF0Qxln"
      },
      "source": [
        "## Before we can actually train the model we need to compile it\n",
        "if OUTPUT_CLASSES == 2:\n",
        "    text_model.compile(loss=\"binary_crossentropy\",\n",
        "                       optimizer=\"adam\",\n",
        "                       metrics=[\"accuracy\"])\n",
        "else:\n",
        "    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                       optimizer=\"adam\",\n",
        "                       metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64-ApuqAQ0Te",
        "outputId": "6c5065a4-1458-429a-9b80-f78122fac0be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text_model.fit(train_data, epochs=NB_EPOCHS)\n",
        "# loss = sum of errors made for each batch in training or validation sets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1407/1407 [==============================] - 233s 165ms/step - loss: 0.3047 - accuracy: 0.8657\n",
            "Epoch 2/5\n",
            "1407/1407 [==============================] - 145s 103ms/step - loss: 0.1296 - accuracy: 0.9525\n",
            "Epoch 3/5\n",
            "1407/1407 [==============================] - 147s 104ms/step - loss: 0.0698 - accuracy: 0.9748\n",
            "Epoch 4/5\n",
            "1407/1407 [==============================] - 146s 104ms/step - loss: 0.0383 - accuracy: 0.9862\n",
            "Epoch 5/5\n",
            "1407/1407 [==============================] - 147s 104ms/step - loss: 0.0176 - accuracy: 0.9937\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5399b77358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvpoH38b-kvS"
      },
      "source": [
        "?tf.keras.Model.evaluate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfOFfGW_RAQ1",
        "outputId": "25a22799-7b9c-447f-b473-2acf90bfeb84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "results = text_model.evaluate(test_data)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "156/156 [==============================] - 15s 95ms/step - loss: 0.6148 - accuracy: 0.8872\n",
            "[0.614834725856781, 0.8872195482254028]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wmv8fT5hUE60"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}